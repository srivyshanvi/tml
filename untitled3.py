# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ukgOx5xMh6C-SzkXOL64pSQ6erPQDSYQ
"""

import numpy as np
import gym
from collections import deque
from keras.models import Sequential
from keras.layers import Dense, Conv2D
from keras.optimizers import Adam
from score.score_logger import ScoreLogger

env_name = "Breakout-v0"
gamma = 0.95
alpha = 0.001

memory_size = 1000000
batch_size = 20

exploration_max = 1.0
exploration_min = 0.01
exploration_decay = 0.995

class DQNsolver:
  def __init__(self, observation_space, action_space):
    self.observation_space = observation_space
    self.exploration_rate = exploration_max
    
    self.action_space = action_space
    self.memory = deque(maxlen=memory_size)
    
    self.model = Sequential()
    self.model.add(Conv2D(32,
                         8,
                         strides = (4, 4),
                         padding = "valid",
                         activation = "relu",
                         input_space = observation_space,
                         data_format = "channels_first"))
    self.model.add(Conv2D(64,
                         4,
                         strides = (2, 2),
                         padding = "valid",
                         activation = "relu",
                         input_space = observation_space,
                         data_format = "channels_first"))
    self.model.add(Conv2D(64,
                         3,
                         strides = (1, 1),
                         padding = "valid",
                         activation = "relu",
                         input_space = observation_space,
                         data_format = "channels_first"))
    self.model.add(Flatten())
    self.model.add(Dense(512, activation = "relu"))
    self.model.add(Dense(self.action_space, activation = "linear"))
    self.model.compile(loss = "mean_square_error",
                      optimizer = Adam(lr = alpha),
                      metrics = ["accuracy"])
  
  def remember(self, state, action, reward, next_state, done):
    self.memory.append((state, action, reward, next_state, done))
  
  def act(self, state):
    if np.random.rand() < self.exploration_rate :
      return random.randrange(self.action_space)
    q_values = self.model.predict(state)
    return np.argmax(q_values[0])
  
  def experience_replay(self) :
    if len(self.memory) < batch_size :
      return
    batch = random.sample(self.memory, bacth_size)
    for state, action, reward, state_next, terminal in batch :
      q_update = reward
      if not terminal :
        q_update = (reward + gamma * np.amax(self.model.predit(state_next)[0]))
      q_values = self.model.predict(state)
      q_values[0][action] = q_update
      self.model.fit(state, q_values, verbose=0)
    self.exploration_rate *= exploration_decay
    self.exploration_rate = max(exploration_min, self.exploration_rate)

def Breakout():
  score_logger = ScoreLogger(env_name)
  observation_space = env.observation_space.shape[0]
  action_space = env.action_space.n
  dqn_solver = DQNsolver(observation_space, action_space)
  run = 0
  while True :
    run += 1
    state = env.reset()
    state = np.reshape(state, [1, observation_space])
    step = 0
    while True :
      step += 1
      env_render()
      action = dqn_solver.act(state)
      state_next, reward, terminal, info = env.step(action)
      dqn_solver.remember(state, action, reward, state_next, terminal)
      state = state_next
      if terminal :
        print("run : " + str(run) + ", exploration : " + str(dqn_solver.exploration_rate) + ", score : " + str(step))
        #score_logger.add_score(step, run)
        break
      dqn_solver.experience_replay()

if __name__=="__main__" :
  Breakout()